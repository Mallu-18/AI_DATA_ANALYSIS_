{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensuring Feature Consistency Between Training & InferencePipelines:\n",
    "\n",
    "**Task 1**: Consistent Feature Preparation\n",
    "- Step 1: Write a function for data preprocessing and imputation shared by both training and inference pipelines.\n",
    "- Step 2: Demonstrate consistent application on both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "def preprocess_data(df, is_training=True, imputer=None, scaler=None):\n",
    "    df_processed = df.copy()\n",
    "    numerical_cols = df_processed.select_dtypes(include=np.number).columns\n",
    "    if is_training:\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        df_processed[numerical_cols] = imputer.fit_transform(df_processed[numerical_cols])\n",
    "    elif imputer is not None:\n",
    "        df_processed[numerical_cols] = imputer.transform(df_processed[numerical_cols])\n",
    "    else:\n",
    "        raise ValueError(\"Imputer must be provided for inference.\")\n",
    "    if is_training:\n",
    "        scaler = StandardScaler()\n",
    "        df_processed[numerical_cols] = scaler.fit_transform(df_processed[numerical_cols])\n",
    "    elif scaler is not None:\n",
    "        df_processed[numerical_cols] = scaler.transform(df_processed[numerical_cols])\n",
    "    else:\n",
    "        raise ValueError(\"Scaler must be provided for inference.\")\n",
    "    return df_processed, imputer, scaler\n",
    "housing_train = fetch_california_housing(as_frame=True)\n",
    "df_train = housing_train.frame.copy()\n",
    "df_train = df_train.iloc[:1000] \n",
    "X_train = df_train.drop('MedHouseVal', axis=1)\n",
    "y_train = df_train['MedHouseVal']\n",
    "X_train_processed, imputer, scaler = preprocess_data(X_train)\n",
    "print(\"Processed Training Data:\")\n",
    "print(X_train_processed.head())\n",
    "print(\"\\nTraining Data Imputer:\", imputer)\n",
    "print(\"Training Data Scaler:\", scaler)\n",
    "housing_inference = fetch_california_housing(as_frame=True)\n",
    "df_inference = housing_inference.frame.copy()\n",
    "df_inference = df_inference.iloc[1000:1100].copy() \n",
    "df_inference.iloc[[5, 15], 0] = np.nan\n",
    "df_inference = df_inference.drop('MedHouseVal', axis=1, errors='ignore')\n",
    "X_inference_processed, _, _ = preprocess_data(df_inference, is_training=False, imputer=imputer, scaler=scaler)\n",
    "print(\"\\nProcessed Inference Data:\")\n",
    "print(X_inference_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Pipeline Integration\n",
    "- Step 1: Use sklearn pipelines to encapsulate the preprocessing steps.\n",
    "- Step 2: Configure identical pipelines for both training and building inference models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "housing_train = fetch_california_housing(as_frame=True)\n",
    "df_train = housing_train.frame.copy()\n",
    "X_train = df_train.drop('MedHouseVal', axis=1)\n",
    "y_train = df_train['MedHouseVal']\n",
    "X_train_sample, _, y_train_sample, _ = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "numerical_features = X_train_sample.select_dtypes(include=np.number).columns\n",
    "preprocessor = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "train_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "train_pipeline.fit(X_train_sample, y_train_sample)\n",
    "housing_inference = fetch_california_housing(as_frame=True)\n",
    "df_inference = housing_inference.frame.copy()\n",
    "X_inference = df_inference.drop('MedHouseVal', axis=1, errors='ignore').iloc[:50]\n",
    "X_inference.iloc[[5, 15], 0] = np.nan\n",
    "inference_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', train_pipeline.named_steps['preprocessor'])\n",
    "])\n",
    "X_inference_processed = inference_pipeline.transform(X_inference)\n",
    "print(\"Processed Inference Data:\")\n",
    "print(X_inference_processed[:5])\n",
    "y_pred_train = train_pipeline.predict(X_train_sample)\n",
    "mse_train = mean_squared_error(y_train_sample, y_pred_train)\n",
    "print(f\"\\nMean Squared Error on Training Data: {mse_train:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Saving and Loading Preprocessing Models\n",
    "- Step 1: Save the transformation model after fitting it to the training data.\n",
    "- Step 2: Load and apply the saved model during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing_train = fetch_california_housing(as_frame=True)\n",
    "df_train = housing_train.frame.copy()\n",
    "X_train = df_train.drop('MedHouseVal', axis=1)\n",
    "y_train = df_train['MedHouseVal']\n",
    "X_train_sample, _, _, _ = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputer.fit(X_train_sample)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_sample)\n",
    "joblib.dump(imputer, 'imputer.joblib')\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "print(\"Preprocessing models (imputer.joblib, scaler.joblib) saved successfully.\")\n",
    "housing_inference = fetch_california_housing(as_frame=True)\n",
    "df_inference = housing_inference.frame.copy()\n",
    "X_inference = df_inference.drop('MedHouseVal', axis=1, errors='ignore').iloc[:50]\n",
    "X_inference.iloc[[5, 15], 0] = np.nan\n",
    "loaded_imputer = joblib.load('imputer.joblib')\n",
    "loaded_scaler = joblib.load('scaler.joblib')\n",
    "print(\"\\nPreprocessing models loaded successfully.\")\n",
    "numerical_cols_inference = X_inference.select_dtypes(include=np.number).columns\n",
    "X_inference_imputed = pd.DataFrame(loaded_imputer.transform(X_inference[numerical_cols_inference]),\n",
    "                                   columns=numerical_cols_inference, index=X_inference.index)\n",
    "X_inference_scaled = pd.DataFrame(loaded_scaler.transform(X_inference_imputed),\n",
    "                                  columns=numerical_cols_inference, index=X_inference_imputed.index)\n",
    "\n",
    "print(\"\\nProcessed Inference Data:\")\n",
    "print(X_inference_scaled.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
