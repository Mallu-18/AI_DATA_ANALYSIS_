{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values in Large-scale ML Pipelines:\n",
    "\n",
    "**Task 1**: Impute with Mean or Median\n",
    "- Step 1: Load a dataset with missing values (e.g., Boston Housing dataset).\n",
    "- Step 2: Identify columns with missing values.\n",
    "- Step 3: Impute missing values using the mean or median of the respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "df = pd.DataFrame(data, columns=feature_names)\n",
    "df['MEDV'] = target\n",
    "df.iloc[[5, 10, 15], 0] = np.nan\n",
    "df.iloc[[20, 25], 5] = np.nan\n",
    "missing_cols = df.columns[df.isnull().any()].tolist()\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "if 'CRIM' in missing_cols:\n",
    "    df['CRIM_imputed'] = mean_imputer.fit_transform(df[['CRIM']])\n",
    "if 'RM' in missing_cols:\n",
    "    df['RM_imputed'] = mean_imputer.fit_transform(df[['RM']])\n",
    "X = df[['CRIM_imputed' if 'CRIM' in missing_cols else 'CRIM', 'RM_imputed' if 'RM' in missing_cols else 'RM']]\n",
    "y = df['MEDV']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error on the test set: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Impute with the Most Frequent Value\n",
    "- Step 1: Use the Titanic dataset and identify columns with missing values.\n",
    "- Step 2: Impute categorical columns using the most frequent value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "missing_cols = df.columns[df.isnull().any()].tolist()\n",
    "categorical_cols_with_missing = df[missing_cols].select_dtypes(include=['object']).columns.tolist()\n",
    "most_frequent_imputer = SimpleImputer(strategy='most_frequent')\n",
    "for col in categorical_cols_with_missing:\n",
    "    df[col + '_imputed'] = most_frequent_imputer.fit_transform(df[[col]]).flatten()\n",
    "print(df[['Embarked', 'Embarked_imputed', 'Cabin', 'Cabin_imputed']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Advanced Imputation - k-Nearest Neighbors\n",
    "- Step 1: Implement KNN imputation using the KNNImputer from sklearn.\n",
    "- Step 2: Explore how KNN imputation improves data completion over simpler methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "# write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "df = housing.frame\n",
    "df.iloc[[5, 10, 15], 0] = np.nan\n",
    "df.iloc[[20, 25], 5] = np.nan\n",
    "missing_cols = df.columns[df.isnull().any()].tolist()\n",
    "X = df.drop('MedHouseVal', axis=1)\n",
    "y = df['MedHouseVal']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "X_train_mean_imputed = mean_imputer.fit_transform(X_train_scaled)\n",
    "X_test_mean_imputed = mean_imputer.transform(X_test_scaled)\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "X_train_knn_imputed = knn_imputer.fit_transform(X_train_scaled)\n",
    "X_test_knn_imputed = knn_imputer.transform(X_test_scaled)\n",
    "mean_model = LinearRegression()\n",
    "mean_model.fit(X_train_mean_imputed, y_train)\n",
    "mean_pred = mean_model.predict(X_test_mean_imputed)\n",
    "mean_mse = mean_squared_error(y_test, mean_pred)\n",
    "knn_model = LinearRegression()\n",
    "knn_model.fit(X_train_knn_imputed, y_train)\n",
    "knn_pred = knn_model.predict(X_test_knn_imputed)\n",
    "knn_mse = mean_squared_error(y_test, knn_pred)\n",
    "print(f\"Mean Imputation MSE: {mean_mse:.2f}\")\n",
    "print(f\"KNN Imputation MSE: {knn_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling & Normalization Best Practices:\n",
    "\n",
    "**Task 1**: Standardization\n",
    "- Step 1: Standardize features using StandardScaler.\n",
    "- Step 2: Observe how standardization affects data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "# write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "iris = load_iris(as_frame=True)\n",
    "df = iris.frame\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df[iris.feature_names])\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=iris.feature_names)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.kdeplot(data=df[iris.feature_names])\n",
    "plt.title('Original Data Distribution')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.kdeplot(data=scaled_df)\n",
    "plt.title('Standardized Data Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Original Data Statistics:\")\n",
    "print(df[iris.feature_names].describe())\n",
    "print(\"\\nStandardized Data Statistics:\")\n",
    "print(scaled_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Min-Max Scaling\n",
    "\n",
    "- Step 1: Scale features to lie between 0 and 1 using MinMaxScaler.\n",
    "- Step 2: Compare with standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "# write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "iris = load_iris(as_frame=True)\n",
    "df = iris.frame\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaled_features = min_max_scaler.fit_transform(df[iris.feature_names])\n",
    "min_max_scaled_df = pd.DataFrame(min_max_scaled_features, columns=iris.feature_names)\n",
    "standard_scaler = StandardScaler()\n",
    "standard_scaled_features = standard_scaler.fit_transform(df[iris.feature_names])\n",
    "standard_scaled_df = pd.DataFrame(standard_scaled_features, columns=iris.feature_names)\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.kdeplot(data=df[iris.feature_names])\n",
    "plt.title('Original Data Distribution')\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.kdeplot(data=min_max_scaled_df)\n",
    "plt.title('Min-Max Scaled Data Distribution (0 to 1)')\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.kdeplot(data=standard_scaled_df)\n",
    "plt.title('Standardized Data Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Original Data Statistics:\")\n",
    "print(df[iris.feature_names].describe())\n",
    "print(\"\\nMin-Max Scaled Data Statistics:\")\n",
    "print(min_max_scaled_df.describe())\n",
    "print(\"\\nStandardized Data Statistics:\")\n",
    "print(standard_scaled_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Robust Scaling\n",
    "- Step 1: Scale features using RobustScaler, which is useful for data with outliers.\n",
    "- Step 2: Assess changes in data scaling compared to other scaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "# write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "df = housing.frame[['HouseAge', 'MedInc']]\n",
    "df['HouseAge_outlier'] = df['HouseAge'].apply(lambda x: 100 if x > 60 else x)\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaled = min_max_scaler.fit_transform(df)\n",
    "standard_scaler = StandardScaler()\n",
    "standard_scaled = standard_scaler.fit_transform(df)\n",
    "robust_scaler = RobustScaler()\n",
    "robust_scaled = robust_scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(np.hstack([min_max_scaled, standard_scaled, robust_scaled]),\n",
    "                         columns=['HouseAge_MM', 'MedInc_MM', 'HouseAge_outlier_MM',\n",
    "                                  'HouseAge_Std', 'MedInc_Std', 'HouseAge_outlier_Std',\n",
    "                                  'HouseAge_Rob', 'MedInc_Rob', 'HouseAge_outlier_Rob'])\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.kdeplot(data=df)\n",
    "plt.title('Original Data Distribution')\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.kdeplot(data=scaled_df[['HouseAge_MM', 'MedInc_MM', 'HouseAge_outlier_MM']])\n",
    "plt.title('Min-Max Scaled Distribution')\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.kdeplot(data=scaled_df[['HouseAge_Rob', 'MedInc_Rob', 'HouseAge_outlier_Rob']])\n",
    "plt.title('Robust Scaled Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Original Data Statistics:\")\n",
    "print(df.describe())\n",
    "print(\"\\nMin-Max Scaled Data Statistics:\")\n",
    "print(scaled_df[['HouseAge_MM', 'MedInc_MM', 'HouseAge_outlier_MM']].describe())\n",
    "print(\"\\nStandard Scaled Data Statistics:\")\n",
    "print(scaled_df[['HouseAge_Std', 'MedInc_Std', 'HouseAge_outlier_Std']].describe())\n",
    "print(\"\\nRobust Scaled Data Statistics:\")\n",
    "print(scaled_df[['HouseAge_Rob', 'MedInc_Rob', 'HouseAge_outlier_Rob']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Techniques:\n",
    "### Removing Highly Correlated Features:\n",
    "\n",
    "**Task 1**: Correlation Matrix\n",
    "- Step 1: Compute correlation matrix.\n",
    "- Step 2: Remove highly correlated features (correlation > 0.9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "df = pd.DataFrame(data, columns=feature_names)\n",
    "correlation_matrix = df.corr().abs()\n",
    "upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "highly_correlated_features = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "df_uncorrelated = df.drop(columns=highly_correlated_features)\n",
    "print(\"Original DataFrame shape:\", df.shape)\n",
    "print(\"DataFrame shape after removing highly correlated features:\", df_uncorrelated.shape)\n",
    "print(\"\\nHighly correlated features removed:\", highly_correlated_features)\n",
    "print(\"\\nCorrelation matrix of the remaining features:\")\n",
    "print(df_uncorrelated.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Mutual Information & Variance Thresholds:\n",
    "\n",
    "**Task 2**: Mutual Information\n",
    "- Step 1: Compute mutual information between features and target.\n",
    "- Step 2: Retain features with high mutual information scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "df = housing.frame\n",
    "X = df.drop('MedHouseVal', axis=1)\n",
    "y = df['MedHouseVal']\n",
    "mutual_info = mutual_info_regression(X, y)\n",
    "mutual_info_series = pd.Series(mutual_info, index=X.columns)\n",
    "sorted_mutual_info = mutual_info_series.sort_values(ascending=False)\n",
    "print(\"Mutual Information Scores:\")\n",
    "print(sorted_mutual_info)\n",
    "top_k = 5\n",
    "selected_features_mi = sorted_mutual_info.head(top_k).index.tolist()\n",
    "X_selected_mi = X[selected_features_mi]\n",
    "print(f\"\\nTop {top_k} features based on mutual information: {selected_features_mi}\")\n",
    "print(\"\\nSelected features DataFrame:\")\n",
    "print(X_selected_mi.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Variance Threshold\n",
    "- Step 1: Implement VarianceThreshold to remove features with low variance.\n",
    "- Step 2: Analyze impact on feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "df = housing.frame.drop('MedHouseVal', axis=1)\n",
    "selector = VarianceThreshold(threshold=1.0)\n",
    "selector.fit(df)\n",
    "\n",
    "low_variance_features = df.columns[~selector.get_support()]\n",
    "df_high_variance = df[df.columns[selector.get_support()]]\n",
    "print(\"Original DataFrame shape:\", df.shape)\n",
    "print(\"DataFrame shape after removing low variance features:\", df_high_variance.shape)\n",
    "print(\"\\nLow variance features removed:\", list(low_variance_features))\n",
    "print(\"\\nDataFrame with high variance features:\")\n",
    "print(df_high_variance.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
