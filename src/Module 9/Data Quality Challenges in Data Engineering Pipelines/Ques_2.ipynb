{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Handling Schema Mismatches using Spark\n",
    "**Description**: Use Apache Spark to address schema mismatches by transforming data to match\n",
    "the expected schema.\n",
    "\n",
    "**Steps**:\n",
    "1. Create Spark session\n",
    "2. Load dataframe\n",
    "3. Define the expected schema\n",
    "4. Handle schema mismatches\n",
    "5. Show corrected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here\n",
    "\n",
    "JAVA_HOME is not set\n",
    "---------------------------------------------------------------------------\n",
    "PySparkRuntimeError                       Traceback (most recent call last)\n",
    "Cell In[6], line 70\n",
    "     67     return df # Return the dataframe\n",
    "     69 if __name__ == \"__main__\":\n",
    "---> 70     handle_schema_mismatch()\n",
    "\n",
    "Cell In[6], line 11, in handle_schema_mismatch()\n",
    "      7 \"\"\"\n",
    "      8 Handles schema mismatches in a Spark DataFrame.\n",
    "      9 \"\"\"\n",
    "     10 # 1. Create Spark session\n",
    "---> 11 spark = SparkSession.builder.appName(\"SchemaMismatchHandler\").getOrCreate()\n",
    "     13 # 2. Load dataframe (simulating a mismatch)\n",
    "     14 data = [\n",
    "     15     (\"101\", \"Alice\", \"2023-01-15\", \"25\", \"50000.00\"),\n",
    "     16     (\"102\", \"Bob\", \"2023-02-20\", \"30\", \"60000\"),\n",
    "   (...)\n",
    "     22     (\"108\", \"Henry\", \"2024-01-01 12:00:00\", \"28\", \"55000.00\"), # Datetime String\n",
    "     23 ]\n",
    "\n",
    "File ~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:497, in SparkSession.Builder.getOrCreate(self)\n",
    "    495     sparkConf.set(key, value)\n",
    "    496 # This SparkContext may be an existing one.\n",
    "--> 497 sc = SparkContext.getOrCreate(sparkConf)\n",
    "    498 # Do not update `SparkConf` for existing `SparkContext`, as it's shared\n",
    "    499 # by all sessions.\n",
    "    500 session = SparkSession(sc, options=self._options)\n",
    "\n",
    "File ~/.local/lib/python3.10/site-packages/pyspark/context.py:515, in SparkContext.getOrCreate(cls, conf)\n",
    "    513 with SparkContext._lock:\n",
    "    514     if SparkContext._active_spark_context is None:\n",
    "--> 515         SparkContext(conf=conf or SparkConf())\n",
    "    516     assert SparkContext._active_spark_context is not None\n",
    "    517     return SparkContext._active_spark_context\n",
    "\n",
    "File ~/.local/lib/python3.10/site-packages/pyspark/context.py:201, in SparkContext.__init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\n",
    "    195 if gateway is not None and gateway.gateway_parameters.auth_token is None:\n",
    "    196     raise ValueError(\n",
    "    197         \"You are trying to pass an insecure Py4j gateway to Spark. This\"\n",
    "    198         \" is not allowed as it is a security risk.\"\n",
    "    199     )\n",
    "--> 201 SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n",
    "    202 try:\n",
    "    203     self._do_init(\n",
    "    204         master,\n",
    "    205         appName,\n",
    "   (...)\n",
    "    215         memory_profiler_cls,\n",
    "    216     )\n",
    "\n",
    "File ~/.local/lib/python3.10/site-packages/pyspark/context.py:436, in SparkContext._ensure_initialized(cls, instance, gateway, conf)\n",
    "    434 with SparkContext._lock:\n",
    "    435     if not SparkContext._gateway:\n",
    "--> 436         SparkContext._gateway = gateway or launch_gateway(conf)\n",
    "    437         SparkContext._jvm = SparkContext._gateway.jvm\n",
    "    439     if instance:\n",
    "\n",
    "File ~/.local/lib/python3.10/site-packages/pyspark/java_gateway.py:107, in launch_gateway(conf, popen_kwargs)\n",
    "    104     time.sleep(0.1)\n",
    "    106 if not os.path.isfile(conn_info_file):\n",
    "--> 107     raise PySparkRuntimeError(\n",
    "    108         error_class=\"JAVA_GATEWAY_EXITED\",\n",
    "    109         message_parameters={},\n",
    "    110     )\n",
    "    112 with open(conn_info_file, \"rb\") as info:\n",
    "    113     gateway_port = read_int(info)\n",
    "\n",
    "PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Detect and Correct Incomplete Data in ETL\n",
    "**Description**: Use Python and Pandas to detect incomplete data in an ETL process and fill\n",
    "missing values with estimates.\n",
    "\n",
    "**Steps**:\n",
    "1. Detect incomplete data\n",
    "2. Fill missing values\n",
    "3. Report changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
